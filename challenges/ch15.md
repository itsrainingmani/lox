# Chapter 15 - A Virtual Machine | Challenges

1. What bytecode instruction sequences would you generate for the following expressions:

```
1 * 2 + 3
1 + 2 * 3
3 - 2 - 1
1 + 2 * 3 - 4 / -5
```

It becomes easier to reason about this if we use () to properly see operator precendence in action. Lox specifies a left-to-right evaluation order, so we do a post-order traversal on each expression.

a. `((1 * 2) + 3)`

```
OP_CONSTANT 1
OP_CONSTANT 2
OP_MUL
OP_CONSTANT 3
OP_ADD
```

b. `(1 + (2 * 3))`

```
OP_CONSTANT 1
OP_CONSTANT 2
OP_CONSTANT 3
OP_MUL
OP_ADD
```

c. `((1 + (2 * 3)) - (4 / (-5)))`

```
OP_CONSTANT 1
OP_CONSTANT 2
OP_CONSTANT 3
OP_MUL
OP_ADD
OP_CONSTANT 4
OP_CONSTANT 5
OP_NEGATE
OP_DIVIDE
OP_SUBTRACT
```

2. (Remember that Lox does not have a syntax for negative number literals, so the -5 is negating the number 5.)

If we really wanted a minimal instruction set, we could eliminate either `OP_NEGATE` or `OP_SUBTRACT`. Show the bytecode instruction sequence you would generate for:

`4 - 3 * -2`
First, without using `OP_NEGATE`. Then, without using `OP_SUBTRACT`.

Given the above, do you think it makes sense to have both instructions? Why or why not? Are there any other redundant instructions you would consider including?

`(4 - (3 * (-2)))`

Which gives us -

```
OP_CONSTANT 4
OP_CONSTANT 3
OP_CONSTANT 2
OP_NEGATE
OP_MUL
OP_SUBTRACT
```

If we didn't want to use `OP_NEGATE`, we use subtract but with 0 as our minuend.

```
OP_CONSTANT 4
OP_CONSTANT 3
OP_CONSTANT 0 // new
OP_CONSTANT 2
OP_SUBTRACT   // instead of OP_NEGATE
OP_MUL
OP_SUBTRACT
```

If we don't want to use `OP_SUBTRACT`, we have to use `OP_NEGATE` on the 2nd operand to `-` and change the op to `+`, so `4 - 3` would become `4 + (-3)`

```
OP_CONSTANT 4
OP_CONSTANT 3
OP_CONSTANT 2
OP_NEGATE
OP_MUL
OP_NEGATE      // new
OP_ADD         // new
```

Since negation and subtraction are pretty common operations, it makes sense to have dedicate op codes for both. We can have a max of 256 opcodes given our single-byte opcode system and it seems worth it to use 2 for common ops.

Some other reasons - high overhead for dispatching so makes sense to keep instructions as high level as possible. Encode common ops as single instructions.

"Other redundant instructions" -> I think it would make sense to consider what are other common operations that one can perform on a number and add them as opcodes. (increment, decrement) for loop traversal seems pretty useful even though they could be encoded as `CONST num, CONST 1, OP_ADD`.

3. Our VM’s stack has a fixed size, and we don’t check if pushing a value overflows it. This means the wrong series of instructions could cause our interpreter to crash or go into undefined behavior. Avoid that by dynamically growing the stack as needed.

What are the costs and benefits of doing so?

4. To interpret `OP_NEGATE`, we pop the operand, negate the value, and then push the result. That’s a simple implementation, but it increments and decrements `stackTop` unnecessarily, since the stack ends up the same height in the end. It might be faster to simply negate the value in place on the stack and leave `stackTop` alone. Try that and see if you can measure a performance difference.

Using `hyperfine` to measure the performance of negation using `push(-pop())`, we get:

```shell
Benchmark 1: ./clox
  Time (mean ± σ):     609.4 µs ±  50.1 µs    [User: 170.1 µs, System: 257.8 µs]
  Range (min … max):   558.0 µs … 1483.3 µs    2749 runs
```

Now if we directly manipulate the value on the top of the stack and measure it we get:

```shell
Benchmark 1: ./clox
  Time (mean ± σ):     615.7 µs ±  73.8 µs    [User: 171.8 µs, System: 261.4 µs]
  Range (min … max):   562.1 µs … 1976.5 µs    1517 runs
```

It's mostly the same with some minor variation. However the max time increased from `1483.3 µs` to `1976.5 µs`. But the average time is not very conclusive. We could try applying a lot of `OP_NEGATE` ops in our chunk to see if the overhead of pushing and popping to the stack becomes visible. We add a constant to our chunk and then negate it 10,000 times. With `push(-pop())`, our benchmark gives us:

```shell
hyperfine -N --warmup 5 './clox'

Benchmark 1: ./clox
  Time (mean ± σ):     633.9 µs ±  57.7 µs    [User: 188.2 µs, System: 263.9 µs]
  Range (min … max):   577.4 µs … 1374.3 µs    2314 runs
```

Negating value in place on the stack:

```shell
hyperfine -N --warmup 5 './clox'

Benchmark 1: ./clox
  Time (mean ± σ):     644.5 µs ±  62.4 µs    [User: 190.0 µs, System: 269.4 µs]
  Range (min … max):   580.5 µs … 1414.1 µs    2121 runs
```

Hmmmm. It seems like there is a very, very minimal cost to directly negating the stack in place. If we increase the number of negate ops to 1million, both the approaches end up taking the same amount of time - `2.7 ms ±   0.1 ms`

Are there other instructions where you can do a similar optimization?
